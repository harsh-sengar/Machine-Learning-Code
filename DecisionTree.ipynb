{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as d\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(val, *boundaries): #functions to convert continous data to labelled data\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node:\n",
    "    def __init__(self, n, ent, pent, l, df, y):\n",
    "        self.name=n\n",
    "        self.bestFeature = None\n",
    "        self.entropy = ent\n",
    "        self.parentEntropy = pent\n",
    "        self.level=l\n",
    "        self.children = []\n",
    "        self.dataframe = df\n",
    "        self.target = y\n",
    "        \n",
    "    def addchild(self,obj):\n",
    "        self.children.append(obj)\n",
    "        \n",
    "    def getentropy(self):\n",
    "        return self.entropy\n",
    "    \n",
    "    def getlevel(self):\n",
    "        return self.level\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTrue(df,feature_name,val):\n",
    "    x = df[feature_name] == val\n",
    "    count = 0\n",
    "    for i in x:\n",
    "        if i == True:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countY(y, val):\n",
    "    count = 0\n",
    "    for i in y:\n",
    "        if i == val:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropyCalc(y):\n",
    "    pc = y.apply(pd.value_counts)\n",
    "    pc = np.array(pc)\n",
    "    total = pc.sum()\n",
    "    #print(total)\n",
    "    entropy = 0\n",
    "    for i in pc:\n",
    "        #print (i)\n",
    "        if(i>0):\n",
    "            entropy = entropy + (-1*(i/total)*math.log((i/total),2))\n",
    "        #print(entropy)\n",
    "    return entropy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightedEntropy(df, y,feature):\n",
    "    if(feature == None):\n",
    "        return entropyCalc(y)\n",
    "    \n",
    "    values = set(df[feature])\n",
    "    \n",
    "    val = []  #putting distinct values in a list because set can be accessed by index\n",
    "    for i in values:\n",
    "        val.append(i)\n",
    "    #print(val)\n",
    "    #caluculating weights of distinct values in df[feature] and putting them in dictionary weight\n",
    "        \n",
    "    weight = {}\n",
    "    for i in val:\n",
    "        weight[i] = countTrue(df,feature,i)/df[feature].shape[0]\n",
    "    #print(weight)\n",
    "    # putting entropy corresponding to distinct values in a certain feature in dictionary called entropy\n",
    "    \n",
    "    entropy = {}    \n",
    "    for i in val:\n",
    "        entropy[i] = entropyCalc(y[df[feature]==i])\n",
    "    #print(entropy)\n",
    "    \n",
    "    #calculating weighted entropy... \n",
    "        \n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    for i in val:\n",
    "        weighted_entropy = weighted_entropy + (weight[i]*entropy[i])\n",
    "        \n",
    "    return weighted_entropy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(parent_entropy, child_entropy):\n",
    "    return parent_entropy - child_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctVal(y):\n",
    "    distinct = set(y)\n",
    "    return len(distinct)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTree(df, y, unused_features, cur_node):\n",
    "    #base case\n",
    "    # 1. unused is empty\n",
    "    # 2. y contains only one distinct value\n",
    "    if(len(unused_features)==0 or distinctVal(y.output) == 1):\n",
    "         return\n",
    "\n",
    "    #at the end of for loop best_feature will contain the name of feature along which on splitting there will be max info gain.\n",
    "    best_feature = \"\"\n",
    "    max_info_gain = 0\n",
    "    \n",
    "    for f in unused_features:        \n",
    "        weighted_entropy = weightedEntropy(df, y, f)\n",
    "        #print(weighted_entropy)\n",
    "        #since cur_node will act as parent oon splitting along selected feature\n",
    "        parent_entropy =cur_node.getentropy()      \n",
    "        info_gain = informationGain(parent_entropy, weighted_entropy)\n",
    "        if(info_gain > max_info_gain):\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = f\n",
    "            \n",
    "    print(\"Best Feature \", best_feature)  \n",
    "    unused_features.remove(best_feature)\n",
    "    possible_values = set(df[best_feature])\n",
    "    cur_node.bestFeature= best_feature\n",
    "    for val in possible_values:\n",
    "        #print(val)\n",
    "        new_y=y[df[best_feature]==val]\n",
    "        new_df=df[df[best_feature]==val]\n",
    "        new_ent=entropyCalc(new_y)  \n",
    "        #creating a node corresponding to each of the distinct values of df[best_feature]\n",
    "        new_node=node(val,new_ent,cur_node.getentropy(),cur_node.getlevel()+1,new_df,new_y)\n",
    "        new_node.bestFeature = best_feature\n",
    "        cur_node.addchild(new_node)\n",
    "        \n",
    "    for child in cur_node.children:\n",
    "        new_df = child.dataframe\n",
    "        new_y = child.target\n",
    "        buildTree(new_df,new_y,unused_features,child)\n",
    "    \n",
    "    # remove best feature from unused features\n",
    "    # loop over possible values of best feature\n",
    "    # call build tree recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeBuilder(df, y, features):\n",
    "    #since root has no parent, keeping its parent's entropy same as that of the root\n",
    "    current_node = node('root', entropyCalc(y), entropyCalc(y), 0, df, y)\n",
    "    \n",
    "    #call build tree\n",
    "    buildTree(df, y, features, current_node)\n",
    "    \n",
    "    print(\"Level \",current_node.level)\n",
    "    possible_outputs = set(current_node.target.output)\n",
    "    for x in possible_outputs:\n",
    "        print(\"Count of \",x,\" = \",countY(current_node.target.output, x))\n",
    "    print(\"Current Entropy is = \",current_node.getentropy())\n",
    "    weighted_entropy = weightedEntropy(current_node.dataframe, current_node.target, current_node.bestFeature)\n",
    "    print(\"Splitting on feature \",current_node.bestFeature,\" with gain ratio \",informationGain(current_node.parentEntropy, weighted_entropy))\n",
    "    print()\n",
    "    \n",
    "    printTree(current_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(current_node):#dfs\n",
    "    for child in current_node.children:\n",
    "        print(\"Level \",child.level)\n",
    "        ss=list(child.target)\n",
    "        f=False\n",
    "        if(child.bestFeature != None):\n",
    "                f=True\n",
    "            \n",
    "  \n",
    "        possible_outputs = set(child.target[ss[0]])\n",
    "        lst=list(child.target[ss[0]])\n",
    "        for val in possible_outputs:\n",
    "            print(\"Count of \",val,\" = \",countY(lst,val))\n",
    "        print(\"Current Entropy  is = \",child.getentropy())\n",
    "        \n",
    "        if(f==True and child.getentropy()!=0):\n",
    "            weighted_entropy = weightedEntropy(child.dataframe,child.target,child.bestFeature)\n",
    "            info_gain = informationGain(current_node.entropy, weighted_entropy)\n",
    "            print(\"Splitting on feature \",child.bestFeature,\" with gain ratio \",info_gain)\n",
    "            print()\n",
    "        elif(f==False or child.getentropy()==0):\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "        printTree(child)\n",
    "         \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreesonIris():\n",
    "    iris = d.load_iris()\n",
    "    df = pd.DataFrame(iris.data)\n",
    "    y = pd.DataFrame(iris.target)\n",
    "\n",
    "    #giving headers to data columns\n",
    "    df.columns = [\"sl\", \"sw\", \"pl\", \"pw\"]\n",
    "    y.columns = ['output']\n",
    "    \n",
    "    #converting continuous data to labelled data\n",
    "    df['sl_labeled'] = toLabel(df, 'sl')\n",
    "    df['sw_labeled'] = toLabel(df, 'sw')\n",
    "    df['pl_labeled'] = toLabel(df, 'pl')\n",
    "    df['pw_labeled'] = toLabel(df, 'pw')\n",
    "    \n",
    "    #dropping original columns\n",
    "    df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)\n",
    "    \n",
    "    #feature list\n",
    "    attributes = []\n",
    "    for j in df.columns:\n",
    "        attributes.append(j)\n",
    "    #splitting data\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(df, y)\n",
    "    \n",
    "    #call decision tree builder\n",
    "    decisionTreeBuilder(df, y, attributes)\n",
    "   # print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Feature  pw_labeled\n",
      "Best Feature  pl_labeled\n",
      "Best Feature  sl_labeled\n",
      "Best Feature  sw_labeled\n",
      "Level  0\n",
      "Count of  0  =  50\n",
      "Count of  1  =  50\n",
      "Count of  2  =  50\n",
      "Current Entropy is =  [1.5849625]\n",
      "Splitting on feature  pw_labeled  with gain ratio  [1.26273082]\n",
      "\n",
      "Level  1\n",
      "Count of  2  =  34\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  1  =  40\n",
      "Count of  2  =  16\n",
      "Current Entropy  is =  [0.86312057]\n",
      "Splitting on feature  pl_labeled  with gain ratio  [1.03255197]\n",
      "\n",
      "Level  2\n",
      "Count of  2  =  8\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  39\n",
      "Count of  2  =  8\n",
      "Current Entropy  is =  [0.65819127]\n",
      "Splitting on feature  sl_labeled  with gain ratio  [0.36283741]\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  2\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  23\n",
      "Count of  2  =  7\n",
      "Current Entropy  is =  [0.78377695]\n",
      "Splitting on feature  sw_labeled  with gain ratio  [-0.03750642]\n",
      "\n",
      "Level  4\n",
      "Count of  1  =  6\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of  1  =  3\n",
      "Count of  2  =  1\n",
      "Current Entropy  is =  [0.81127812]\n",
      "Splitting on feature  sw_labeled  with gain ratio  [-0.02750118]\n",
      "\n",
      "Level  4\n",
      "Count of  1  =  14\n",
      "Count of  2  =  6\n",
      "Current Entropy  is =  [0.8812909]\n",
      "Splitting on feature  sw_labeled  with gain ratio  [-0.09751395]\n",
      "\n",
      "Level  3\n",
      "Count of  2  =  1\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  14\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  1\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  0  =  50\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  1  =  10\n",
      "Current Entropy  is =  [0.]\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DecisionTreesonIris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
